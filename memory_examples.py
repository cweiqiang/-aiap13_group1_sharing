# -*- coding: utf-8 -*-
"""memory_examples.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kPbd7r1fStbENh-iH2VbHTLKqmXe4R8P

`Pip Install Required Packages`
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain openai networkx

"""`Importing Packages`

"""

from langchain import OpenAI
from langchain.chains import LLMChain, ConversationChain
from langchain.chains.conversation.memory import (
    ConversationBufferMemory,
    ConversationSummaryMemory,
    ConversationBufferWindowMemory,
    ConversationKGMemory,
)
from langchain.callbacks import get_openai_callback
from getpass import getpass

"""`Entering OpenAI Secret Key`

To run this notebook, we will need to use an OpenAI LLM. Therefore, you will need to input your openai api key when prompted after running the following cell:

"""

OPENAI_API_KEY = getpass()

"""`Defining LLM parameters`

"""

llm = OpenAI(
    temperature=0,
    openai_api_key=OPENAI_API_KEY,
    model_name="text-davinci-003",  # can be used with llms like 'gpt-3.5-turbo'
)

"""`Defining count_tokens utility function`

The count_tokens utility function will help us determine the total number of tokens that we are using for each call.

"""

def count_tokens(chain, query):
    with get_openai_callback() as cb:
        result = chain.run(query)
        print(f"Spent a total of {cb.total_tokens} tokens")

    return result

"""`Defining and inspecting the conversation chain that we will be using`

This chain combines an input from the user with the conversation history to generate a meaningful response.

"""

conversation = ConversationChain(
    llm=llm,
)

print(conversation.prompt.template)

"""By default, the conversation chain prompt template tells the LLM to give truthful answers to the user during its chat with the user.

`Memory Type 1: ConversationBufferMemory`
"""

conversation_type_1 = ConversationChain(llm=llm, memory=ConversationBufferMemory())

conversation_type_1("Good morning AI!")

conversation_type_1(
    "My interest here is to explore the potential of integrating Large Language Models with external knowledge"
)

conversation_type_1(
    "I just want to analyze the different possibilities. What can you think of?"
)

conversation_type_1(
    "Which data source types could be used to give context to the model?"
)

conversation_type_1("What is my aim again?")

print(conversation_type_1.memory.buffer)

"""`Memory Type 2: ConversationBufferWindowMemory`

"""

conversation_type_2 = ConversationChain(
    llm=llm, memory=ConversationBufferWindowMemory(k=1)
)

conversation_type_2("Good morning AI!")

conversation_type_2(
    "My interest here is to explore the potential of integrating Large Language Models with external knowledge"
)

conversation_type_2(
    "I just want to analyze the different possibilities. What can you think of?"
)

conversation_type_2(
    "Which data source types could be used to give context to the model?"
)

conversation_type_2("What is my aim again?")

print(conversation_type_2.memory.buffer)

"""`Memory Type 3: ConversationSummaryMemory`

"""

conversation_type_3 = ConversationChain(
    llm=llm, memory=ConversationSummaryMemory(llm=llm)
)

print(conversation_type_3.memory.prompt.template)

conversation_type_3("Good morning AI!")

conversation_type_3(
    "My interest here is to explore the potential of integrating Large Language Models with external knowledge"
)

conversation_type_3(
    "I just want to analyze the different possibilities. What can you think of?"
)

conversation_type_3(
    "Which data source types could be used to give context to the model?"
)

conversation_type_3("What is my aim again?")

print(conversation_type_3.memory.buffer)

"""`Memory Type 4: ConversationKnowledgeGraphMemory`

"""

conversation_type_4 = ConversationChain(llm=llm, memory=ConversationKGMemory(llm=llm))

conversation_type_4("My name is human and I like coffee!")

conversation_type_4("Yes, I also like tea! What beverage do you like?")

conversation_type_4.memory.kg.get_triples()